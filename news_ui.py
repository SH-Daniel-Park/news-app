# -*- coding: utf-8 -*-
import io
import json
import pandas as pd
import streamlit as st
import datetime as dt

from news_aggregator import (
    collect_articles,
    enrich_with_content,
    filter_by_publishers,
    extract_domain,
    DEFAULT_RSS_FEEDS,
)

st.set_page_config(page_title="ğŸ“° ë‰´ìŠ¤ í‚¤ì›Œë“œ ìˆ˜ì§‘/ìš”ì•½ ëŒ€ì‹œë³´ë“œ", layout="wide")
st.title("ğŸ“° ë‰´ìŠ¤ í‚¤ì›Œë“œ ìˆ˜ì§‘/ìš”ì•½ ëŒ€ì‹œë³´ë“œ")
st.caption("í‚¤ì›Œë“œë¡œ ì—¬ëŸ¬ ì–¸ë¡  ê¸°ì‚¬ë¥¼ ëª¨ì•„ë³´ê³ , ë³¸ë¬¸/ìš”ì•½/í‚¤ì›Œë“œë¥¼ í•¨ê»˜ í™•ì¸í•˜ì„¸ìš”.")

# --------------------------- ì‚¬ì´ë“œë°” ---------------------------
with st.sidebar:
    st.header("ê²€ìƒ‰ ì„¤ì •")
    query = st.text_input("ê²€ìƒ‰ í‚¤ì›Œë“œ", placeholder="ì˜ˆ) ì¬ì •ì •ì±…, ë°˜ë„ì²´, í™˜ìœ¨ ê¸‰ë“±")
    max_results = st.slider("ìµœëŒ€ ê¸°ì‚¬ ìˆ˜", 10, 300, 60, step=10)
    newsapi_key = st.text_input("NewsAPI í‚¤ (ì„ íƒ)", type="password")

    st.markdown("---")
    st.subheader("ê¸°ê°„(ì„ íƒ)")
    use_date_range = st.toggle("ê¸°ê°„ í•„í„° ì‚¬ìš©", value=False)
    start_date = end_date = None
    if use_date_range:
        col1, col2 = st.columns(2)
        with col1:
            start_date = st.date_input("ì‹œì‘ì¼", value=dt.date.today())
        with col2:
            end_date = st.date_input("ì¢…ë£Œì¼", value=dt.date.today())

    st.markdown("---")
    st.subheader("RSS ì†ŒìŠ¤")
    use_default_rss = st.checkbox("ìƒ˜í”Œ ê¸°ë³¸ RSS ì‚¬ìš©", True)
    uploaded_feeds = st.file_uploader("feeds.txt ì—…ë¡œë“œ", type=["txt"])

    rss_feeds = []
    if use_default_rss:
        rss_feeds.extend(DEFAULT_RSS_FEEDS)
    if uploaded_feeds is not None:
        try:
            txt = uploaded_feeds.read().decode("utf-8", errors="ignore")
            for line in txt.splitlines():
                url = line.strip()
                if url and not url.startswith("#"):
                    rss_feeds.append(url)
        except Exception:
            st.warning("feeds.txtë¥¼ ì½ëŠ” ì¤‘ ë¬¸ì œê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤.")

    st.markdown("---")
    st.subheader("ì½˜í…ì¸  ì²˜ë¦¬")
    do_fetch_text = st.checkbox("ê¸°ì‚¬ ë³¸ë¬¸ ìˆ˜ì§‘ (í¬ë¡¤ë§)", True)
    do_summarize = st.checkbox("ìš”ì•½ ìƒì„±", True)
    summary_len = st.slider("ìš”ì•½ ë¬¸ì¥ ìˆ˜", 2, 6, 3)
    do_keywords = st.checkbox("í‚¤ì›Œë“œ(í˜•íƒœì†Œ) ì¶”ì¶œ", True)

    st.markdown("---")
    run = st.button("ğŸ” ìˆ˜ì§‘ ì‹œì‘", use_container_width=True)

# --------------------------- ì‹¤í–‰ ---------------------------
if run:
    if not query.strip():
        st.warning("í‚¤ì›Œë“œë¥¼ ì…ë ¥í•˜ì„¸ìš”.")
        st.stop()

    with st.spinner("ê¸°ì‚¬ë¥¼ ìˆ˜ì§‘í•˜ê³  ìˆìŠµë‹ˆë‹¤..."):
        raw = collect_articles(
            query=query,
            max_results=max_results,
            newsapi_key=newsapi_key.strip() or None,
            rss_feeds=rss_feeds if rss_feeds else None,
        )

    if not raw:
        st.info("ê´€ë ¨ ê¸°ì‚¬ë¥¼ ì°¾ì§€ ëª»í–ˆìŠµë‹ˆë‹¤.")
        st.stop()

    publishers = sorted(list({(it.get("publisher") or extract_domain(it.get("link","")) or "").strip()
                              for it in raw if it.get("link")}))
    with st.expander("ì–¸ë¡ ì‚¬/ë„ë©”ì¸ í•„í„°"):
        allow = st.multiselect("í¬í•¨í•  ì–¸ë¡ ì‚¬/ë„ë©”ì¸", options=publishers, default=[])

    filtered = filter_by_publishers(raw, allow_publishers=allow)

    # ê¸°ê°„ í•„í„°
    def parse_date_iso(s):
        try:
            return pd.to_datetime(s, utc=True).date()
        except Exception:
            return None

    if use_date_range and (start_date or end_date):
        if start_date and end_date and end_date < start_date:
            start_date, end_date = end_date, start_date
        tmp = []
        for it in filtered:
            d = parse_date_iso(it.get("published_at"))
            ok = True
            if start_date and d and d < start_date:
                ok = False
            if end_date and d and d > end_date:
                ok = False
            if ok:
                tmp.append(it)
        filtered = tmp

    if not filtered:
        st.info("í•„í„° ì¡°ê±´ì— ë§ëŠ” ê¸°ì‚¬ê°€ ì—†ìŠµë‹ˆë‹¤.")
        st.stop()

    # ë³¸ë¬¸/ìš”ì•½/í‚¤ì›Œë“œ
    if do_fetch_text or do_summarize or do_keywords:
        with st.spinner("ë³¸ë¬¸/ìš”ì•½/í‚¤ì›Œë“œë¥¼ ìƒì„± ì¤‘ì…ë‹ˆë‹¤..."):
            enriched = enrich_with_content(
                filtered,
                do_fetch_text=do_fetch_text,
                do_summarize=do_summarize,
                do_keywords=do_keywords,
                summary_sentences=summary_len,
            )
    else:
        enriched = filtered

    df = pd.DataFrame(enriched)
    display_cols = ["title", "publisher", "published_at", "link"]
    if do_summarize:
        display_cols.append("summary")
    if do_keywords:
        display_cols.append("keywords")

    st.success(f"ì´ {len(df)}ê±´ì˜ ê¸°ì‚¬ë¥¼ í™•ë³´í–ˆìŠµë‹ˆë‹¤.")
    st.dataframe(df[display_cols], use_container_width=True, height=520)

    # ---------------- Excel ë‹¤ìš´ë¡œë“œ (ì œëª©ë§Œ í´ë¦­ ê°€ëŠ¥) ----------------
    from io import BytesIO

    # ì—‘ì…€ ë°ì´í„° ì¤€ë¹„: URL ì»¬ëŸ¼ì„ ëª…í™•íˆ 'url'ë¡œ
    df_excel = df.copy()
    if "link" in df_excel.columns:
        df_excel.rename(columns={"link": "url"}, inplace=True)

    # ì—”ì§„ ì„ íƒ
    engine = None
    try:
        import xlsxwriter
        engine = "xlsxwriter"
    except Exception:
        try:
            import openpyxl
            engine = "openpyxl"
        except Exception:
            engine = None

    if engine is None:
        st.error("xlsxwriter ë˜ëŠ” openpyxlì´ í•„ìš”í•©ë‹ˆë‹¤. requirements.txtì— ì¶”ê°€ í›„ ë°°í¬í•˜ì„¸ìš”.")
    else:
        output = BytesIO()
        with pd.ExcelWriter(output, engine=engine) as writer:
            # ë°ì´í„° ë¨¼ì € ê¸°ë¡ (TITLEì€ í™”ë©´ê³¼ ë™ì¼)
            df_excel.to_excel(writer, index=False, sheet_name="results")
            ws = writer.sheets["results"]

            cols = list(df_excel.columns)
            title_idx = cols.index("title") if "title" in cols else None
            url_idx   = cols.index("url")   if "url" in cols else None

            # ì œëª©ë§Œ í´ë¦­ ê°€ëŠ¥í•˜ê²Œ (urlì€ ê·¸ëŒ€ë¡œ ë¬¸ìì—´ ë³´ì¡´)
            if url_idx is not None and title_idx is not None:
                if engine == "xlsxwriter":
                    # xlsxwriterëŠ” 0-based, row 0ì€ í—¤ë”
                    for r, (title, url) in enumerate(zip(df_excel["title"], df_excel["url"]), start=1):
                        if pd.notna(url) and str(url).strip().startswith("http"):
                            ws.write_url(r, title_idx, str(url), string=str(title))
                else:
                    # openpyxlì€ 1-based
                    from openpyxl.styles import Font
                    for r, (title, url) in enumerate(zip(df_excel["title"], df_excel["url"]), start=2):
                        if pd.notna(url) and str(url).strip().startswith("http"):
                            cell = ws.cell(row=r, column=title_idx + 1)
                            cell.value = str(title)
                            cell.hyperlink = str(url)
                            cell.font = Font(color="0000EE", underline="single")

        output.seek(0)
        st.download_button(
            "ì—‘ì…€(.xlsx)ë¡œ ë‹¤ìš´ë¡œë“œ",
            data=output.getvalue(),
            file_name=f"{query}_results.xlsx",
            mime="application/vnd.openxmlformats-officedocument.spreadsheetml.sheet",
            use_container_width=True,
        )
    # -------------------------------------------------------------------

else:
    st.info("ì¢Œì¸¡ì—ì„œ í‚¤ì›Œë“œë¥¼ ì…ë ¥ í›„ ìˆ˜ì§‘ ì‹œì‘ì„ ëˆŒëŸ¬ì£¼ì„¸ìš”.")

# -*- coding: utf-8 -*-
import io
import json
import pandas as pd
import streamlit as st
import datetime as dt

from news_aggregator import (
    collect_articles,
    enrich_with_content,
    filter_by_publishers,
    extract_domain,
    DEFAULT_RSS_FEEDS,
)

st.set_page_config(page_title="ğŸ“° ë‰´ìŠ¤ í‚¤ì›Œë“œ ìˆ˜ì§‘/ìš”ì•½ ëŒ€ì‹œë³´ë“œ", layout="wide")
st.title("ğŸ“° ë‰´ìŠ¤ í‚¤ì›Œë“œ ìˆ˜ì§‘/ìš”ì•½ ëŒ€ì‹œë³´ë“œ")
st.caption("í‚¤ì›Œë“œë¡œ ì—¬ëŸ¬ ì–¸ë¡  ê¸°ì‚¬ë¥¼ ëª¨ì•„ë³´ê³ , ë³¸ë¬¸/ìš”ì•½/í‚¤ì›Œë“œë¥¼ í•¨ê»˜ í™•ì¸í•˜ì„¸ìš”.")

# --------------------------- ì‚¬ì´ë“œë°” ---------------------------
with st.sidebar:
    st.header("ê²€ìƒ‰ ì„¤ì •")
    query = st.text_input("ê²€ìƒ‰ í‚¤ì›Œë“œ", placeholder="ì˜ˆ) ì¬ì •ì •ì±…, ë°˜ë„ì²´, í™˜ìœ¨ ê¸‰ë“±")
    max_results = st.slider("ìµœëŒ€ ê¸°ì‚¬ ìˆ˜", 10, 300, 60, step=10)
    newsapi_key = st.text_input("NewsAPI í‚¤ (ì„ íƒ)", type="password")

    st.markdown("---")
    st.subheader("ê¸°ê°„(ì„ íƒ)")
    use_date_range = st.toggle("ê¸°ê°„ í•„í„° ì‚¬ìš©", value=False)
    start_date = end_date = None
    if use_date_range:
        col1, col2 = st.columns(2)
        with col1:
            start_date = st.date_input("ì‹œì‘ì¼", value=dt.date.today())
        with col2:
            end_date = st.date_input("ì¢…ë£Œì¼", value=dt.date.today())

    st.markdown("---")
    st.subheader("RSS ì†ŒìŠ¤")
    use_default_rss = st.checkbox("ìƒ˜í”Œ ê¸°ë³¸ RSS ì‚¬ìš©", True)
    uploaded_feeds = st.file_uploader("feeds.txt ì—…ë¡œë“œ", type=["txt"])

    rss_feeds = []
    if use_default_rss:
        rss_feeds.extend(DEFAULT_RSS_FEEDS)
    if uploaded_feeds is not None:
        try:
            txt = uploaded_feeds.read().decode("utf-8", errors="ignore")
            for line in txt.splitlines():
                url = line.strip()
                if url and not url.startswith("#"):
                    rss_feeds.append(url)
        except Exception:
            st.warning("feeds.txtë¥¼ ì½ëŠ” ì¤‘ ë¬¸ì œê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤.")

    st.markdown("---")
    st.subheader("ì½˜í…ì¸  ì²˜ë¦¬")
    do_fetch_text = st.checkbox("ê¸°ì‚¬ ë³¸ë¬¸ ìˆ˜ì§‘ (í¬ë¡¤ë§)", True)
    do_summarize = st.checkbox("ìš”ì•½ ìƒì„±", True)
    summary_len = st.slider("ìš”ì•½ ë¬¸ì¥ ìˆ˜", 2, 6, 3)
    do_keywords = st.checkbox("í‚¤ì›Œë“œ(í˜•íƒœì†Œ) ì¶”ì¶œ", True)

    st.markdown("---")
    st.subheader("PDF í•œê¸€ í°íŠ¸(ì„ íƒ) ì—…ë¡œë“œ")
    pdf_font_file = st.file_uploader("NotoSansKR ë“± .ttf/.otf ì—…ë¡œë“œí•˜ë©´ PDF í•œê¸€ í‘œì‹œê°€ ì¢‹ìŠµë‹ˆë‹¤.", type=["ttf", "otf"])

    st.markdown("---")
    run = st.button("ğŸ” ìˆ˜ì§‘ ì‹œì‘", use_container_width=True)

# --------------------------- ìœ í‹¸ ---------------------------
def _to_yyyymmdd(s: str) -> str:
    if not s or pd.isna(s):
        return ""
    try:
        d = pd.to_datetime(s, errors="coerce", utc=True)
        if pd.isna(d):
            d = pd.to_datetime(s, errors="coerce")
        if pd.isna(d):
            return ""
        return d.strftime("%Y-%m-%d")
    except Exception:
        return ""

def _truncate_kor(s: str, max_chars: int = 8) -> str:
    if s is None:
        return ""
    s = str(s)
    if len(s) <= max_chars:
        return s
    return s[:max_chars] + "â€¦"

def _make_title_text(query, start_date, end_date, use_date_range=False):
    parts = [f"í‚¤ì›Œë“œ: {query.strip()}"]
    if use_date_range and (start_date or end_date):
        s = start_date.strftime("%Y-%m-%d") if start_date else ""
        e = end_date.strftime("%Y-%m-%d") if end_date else ""
        parts.append(f"ê¸°ê°„: {s} ~ {e}")
    return " / ".join([p for p in parts if p])

# --------------------------- ì‹¤í–‰ ---------------------------
if run:
    if not query.strip():
        st.warning("í‚¤ì›Œë“œë¥¼ ì…ë ¥í•˜ì„¸ìš”.")
        st.stop()

    with st.spinner("ê¸°ì‚¬ë¥¼ ìˆ˜ì§‘í•˜ê³  ìˆìŠµë‹ˆë‹¤..."):
        raw = collect_articles(
            query=query,
            max_results=max_results,
            newsapi_key=newsapi_key.strip() or None,
            rss_feeds=rss_feeds if rss_feeds else None,
        )

    if not raw:
        st.info("ê´€ë ¨ ê¸°ì‚¬ë¥¼ ì°¾ì§€ ëª»í–ˆìŠµë‹ˆë‹¤.")
        st.stop()

    publishers = sorted(list({(it.get("publisher") or extract_domain(it.get("link","")) or "").strip()
                              for it in raw if it.get("link")}))
    with st.expander("ì–¸ë¡ ì‚¬/ë„ë©”ì¸ í•„í„°"):
        allow = st.multiselect("í¬í•¨í•  ì–¸ë¡ ì‚¬/ë„ë©”ì¸", options=publishers, default=[])

    filtered = filter_by_publishers(raw, allow_publishers=allow)

    # ê¸°ê°„ í•„í„°
    def parse_date_iso(s):
        try:
            return pd.to_datetime(s, utc=True).date()
        except Exception:
            return None

    if use_date_range and (start_date or end_date):
        if start_date and end_date and end_date < start_date:
            start_date, end_date = end_date, start_date
        tmp = []
        for it in filtered:
            d = parse_date_iso(it.get("published_at"))
            ok = True
            if start_date and d and d < start_date:
                ok = False
            if end_date and d and d > end_date:
                ok = False
            if ok:
                tmp.append(it)
        filtered = tmp

    if not filtered:
        st.info("í•„í„° ì¡°ê±´ì— ë§ëŠ” ê¸°ì‚¬ê°€ ì—†ìŠµë‹ˆë‹¤.")
        st.stop()

    # ë³¸ë¬¸/ìš”ì•½/í‚¤ì›Œë“œ
    if do_fetch_text or do_summarize or do_keywords:
        with st.spinner("ë³¸ë¬¸/ìš”ì•½/í‚¤ì›Œë“œë¥¼ ìƒì„± ì¤‘ì…ë‹ˆë‹¤..."):
            enriched = enrich_with_content(
                filtered,
                do_fetch_text=do_fetch_text,
                do_summarize=do_summarize,
                do_keywords=do_keywords,
                summary_sentences=summary_len,
            )
    else:
        enriched = filtered

    df = pd.DataFrame(enriched)

    # í™”ë©´ í‘œì‹œëŠ” URL ì œì™¸ + ë‚ ì§œ YYYY-MM-DD í‘œì¤€í™”
    df_display = df.copy()
    if "published_at" in df_display.columns:
        df_display["published_at"] = df_display["published_at"].map(_to_yyyymmdd)

    display_cols = ["title", "publisher", "published_at"]
    if do_summarize:
        display_cols.append("summary")
    if do_keywords:
        display_cols.append("keywords")

    st.success(f"ì´ {len(df_display)}ê±´ì˜ ê¸°ì‚¬ë¥¼ í™•ë³´í–ˆìŠµë‹ˆë‹¤.")
    st.dataframe(df_display[display_cols], use_container_width=True, height=520)

    # ë‚´ë¶€ ë§í¬ ì‹œë¦¬ì¦ˆ(ì—‘ì…€/PDFì—ì„œ ì œëª© í•˜ì´í¼ë§í¬ìš©)
    link_series = df["link"] if "link" in df.columns else pd.Series([""] * len(df))

    # ê³µí†µ íƒ€ì´í‹€ í…ìŠ¤íŠ¸
    title_text = _make_title_text(query, start_date, end_date, use_date_range)

    # ---------------- Excel ë‹¤ìš´ë¡œë“œ (ì²« í–‰ ì œëª© + URL ì œì™¸ + ì œëª©ë§Œ í´ë¦­ + ë‚ ì§œ YYYY-MM-DD) ----------------
    from io import BytesIO

    df_excel = df_display[display_cols].copy()  # URL ì œì™¸, ë‚ ì§œ ì´ë¯¸ YYYY-MM-DDë¡œ ì •ê·œí™”

    engine = None
    try:
        import xlsxwriter
        engine = "xlsxwriter"
    except Exception:
        try:
            import openpyxl
            engine = "openpyxl"
        except Exception:
            engine = None

    if engine is None:
        st.error("xlsxwriter ë˜ëŠ” openpyxlì´ í•„ìš”í•©ë‹ˆë‹¤. requirements.txtì— ì¶”ê°€ í›„ ë°°í¬í•˜ì„¸ìš”.")
    else:
        output = BytesIO()
        if engine == "xlsxwriter":
            with pd.ExcelWriter(output, engine=engine) as writer:
                # ë°ì´í„°ëŠ” 2í–‰ë¶€í„° ì“°ê¸° (0-basedë¡œ startrow=1) â†’ 1í–‰(A1)ì€ íƒ€ì´í‹€
                df_excel.to_excel(writer, index=False, sheet_name="results", startrow=1)
                ws = writer.sheets["results"]
                cols = list(df_excel.columns)
                title_idx = cols.index("title") if "title" in cols else None

                # A1 ~ ë§ˆì§€ë§‰ ì»¬ëŸ¼ í—¤ë”ê¹Œì§€ ë³‘í•©í•˜ì—¬ íƒ€ì´í‹€ ë°°ì¹˜
                last_col_letter = chr(ord('A') + len(cols) - 1)
                title_format = writer.book.add_format({
                    "bold": True, "font_size": 12, "align": "left", "valign": "vcenter"
                })
                ws.merge_range(f"A1:{last_col_letter}1", title_text, title_format)

                # ì œëª©ë§Œ í´ë¦­ ê°€ëŠ¥ í•˜ì´í¼ë§í¬ (ë°ì´í„°ëŠ” 2í–‰ë¶€í„° ì‹œì‘ì´ë¯€ë¡œ r=2ë¶€í„° ë³´ì´ê²Œ â†’ 0-basedë¡œ r=2-1=1 í—¤ë”, ë°ì´í„°ëŠ” r>=2)
                # ì‹¤ì œ write_urlì€ 0-based ì¢Œí‘œ: í—¤ë”ëŠ” row=1, ë°ì´í„° ì²« í–‰ì€ row=2
                # enumerate start=2 â†’ Data starts at Excel row index 2 (0-based=1) but we wrote data startrow=1,
                # so hyperlinks must start from row=2 (0-based index=2) to align correctly?
                # Safer: iterate over df_excel with 0-based i and add +2 to row (title row 0, header row 1, data starts at 2).
                for i, (title, url) in enumerate(zip(df_excel["title"], link_series)):
                    if pd.notna(url) and str(url).strip().startswith("http"):
                        ws.write_url(2 + i, title_idx, str(url), string=str(title))
        else:
            # openpyxl path
            with pd.ExcelWriter(output, engine=engine) as writer:
                df_excel.to_excel(writer, index=False, sheet_name="results", startrow=1)
                ws = writer.sheets["results"]

                from openpyxl.styles import Font, Alignment
                cols = list(df_excel.columns)
                title_idx = cols.index("title") if "title" in cols else None

                # ë³‘í•© ë° íƒ€ì´í‹€
                ws.merge_cells(start_row=1, start_column=1, end_row=1, end_column=len(cols))
                cell = ws.cell(row=1, column=1)
                cell.value = title_text
                cell.font = Font(bold=True, size=12)
                cell.alignment = Alignment(horizontal="left", vertical="center")

                # ì œëª© í•˜ì´í¼ë§í¬ (ë°ì´í„°ëŠ” 2í–‰ë¶€í„° â†’ í—¤ë”ê°€ 2í–‰, ë°ì´í„°ëŠ” 3í–‰ë¶€í„°)
                for i, (title, url) in enumerate(zip(df_excel["title"], link_series), start=3):
                    if pd.notna(url) and str(url).strip().startswith("http"):
                        c = ws.cell(row=i, column=title_idx + 1)
                        c.value = str(title)
                        c.hyperlink = str(url)
                        c.font = Font(color="0000EE", underline="single")

        st.download_button(
            "ì—‘ì…€(.xlsx)ë¡œ ë‹¤ìš´ë¡œë“œ",
            data=output.getvalue(),
            file_name=f"{query}_results.xlsx",
            mime="application/vnd.openxmlformats-officedocument.spreadsheetml.sheet",
            use_container_width=True,
        )
    # -------------------------------------------------------------------

    # ---------------- PDF ë‹¤ìš´ë¡œë“œ (ì²« í–‰ ì œëª© + URL ì œì™¸ + ì œëª©ë§Œ ë§í¬ + ë‚ ì§œ YYYY-MM-DD) ---------
    try:
        from reportlab.lib.pagesizes import A4, landscape
        from reportlab.platypus import SimpleDocTemplate, Table, TableStyle, Paragraph, Spacer
        from reportlab.lib import colors
        from reportlab.lib.styles import getSampleStyleSheet, ParagraphStyle
        from reportlab.pdfbase import pdfmetrics
        from reportlab.pdfbase.ttfonts import TTFont
    except Exception:
        st.error("PDF ìƒì„±ì„ ìœ„í•´ reportlab íŒ¨í‚¤ì§€ê°€ í•„ìš”í•©ë‹ˆë‹¤. requirements.txtì— reportlabì„ ì¶”ê°€í•˜ì„¸ìš”.")
    else:
        base_font_name = "Helvetica"
        if pdf_font_file is not None:
            try:
                font_bytes = pdf_font_file.read()
                font_name = "UserKoreanFont"
                import tempfile
                with tempfile.NamedTemporaryFile(delete=False, suffix="."+pdf_font_file.name.split(".")[-1]) as tf:
                    tf.write(font_bytes)
                    tmp_font_path = tf.name
                pdfmetrics.registerFont(TTFont(font_name, tmp_font_path))
                base_font_name = font_name
            except Exception:
                st.warning("ì—…ë¡œë“œí•œ í°íŠ¸ë¥¼ ë“±ë¡í•˜ì§€ ëª»í–ˆìŠµë‹ˆë‹¤. ê¸°ë³¸ í°íŠ¸ë¡œ ì§„í–‰í•©ë‹ˆë‹¤. (í•œê¸€ í‘œì‹œê°€ ê¹¨ì§ˆ ìˆ˜ ìˆìŒ)")

        styles = getSampleStyleSheet()
        title_style_link = ParagraphStyle(
            name="TitleLink",
            parent=styles["Normal"],
            fontName=base_font_name,
            fontSize=10,
            textColor=colors.HexColor("#1155cc"),
            leading=14,
        )
        cell_style = ParagraphStyle(
            name="Cell",
            parent=styles["Normal"],
            fontName=base_font_name,
            fontSize=9,
            leading=12,
        )
        header_style = ParagraphStyle(
            name="Header",
            parent=styles["Normal"],
            fontName=base_font_name,
            fontSize=12,
            leading=16,
            spaceAfter=6,
            textColor=colors.HexColor("#202124"),
        )

        # ìƒë‹¨ íƒ€ì´í‹€
        story = [Paragraph(title_text, header_style), Spacer(1, 6)]

        # í‘œ ë°ì´í„° ì¤€ë¹„
        df_display = df.copy()
        if "published_at" in df_display.columns:
            df_display["published_at"] = df_display["published_at"].map(_to_yyyymmdd)

        rows = []
        header = ["ì œëª©", "ì–¸ë¡ ì‚¬", "ë°œí–‰ì¼"]
        rows.append(header)

        for idx, row in df_display.iterrows():
            title = str(row.get("title", ""))
            url = str(df.loc[idx].get("link", "")) if "link" in df.columns else ""
            pub_full = str(row.get("publisher", ""))
            pub_trunc = _truncate_kor(pub_full, 8)
            when = str(row.get("published_at", ""))  # YYYY-MM-DD

            if url.startswith("http"):
                title_para = Paragraph(f'<link href="{url}">{title}</link>', title_style_link)
            else:
                title_para = Paragraph(title, cell_style)

            pub_para = Paragraph(pub_trunc, cell_style)
            when_para = Paragraph(when, cell_style)

            rows.append([title_para, pub_para, when_para])

        buffer = io.BytesIO()
        doc = SimpleDocTemplate(buffer, pagesize=landscape(A4), leftMargin=25, rightMargin=25, topMargin=20, bottomMargin=20)
        tbl = Table(rows, repeatRows=1, colWidths=[340, 90, 150])
        tbl.setStyle(TableStyle([
            ("BACKGROUND", (0,0), (-1,0), colors.HexColor("#f1f3f4")),
            ("TEXTCOLOR", (0,0), (-1,0), colors.HexColor("#202124")),
            ("FONTNAME", (0,0), (-1,-1), base_font_name),
            ("FONTSIZE", (0,0), (-1,0), 10),
            ("FONTSIZE", (0,1), (-1,-1), 9),
            ("ALIGN", (2,1), (2,-1), "CENTER"),
            ("VALIGN", (0,0), (-1,-1), "TOP"),
            ("ROWBACKGROUNDS", (0,1), (-1,-1), [colors.white, colors.HexColor("#fafafa")]),
            ("GRID", (0,0), (-1,-1), 0.25, colors.HexColor("#dfe1e5")),
        ]))

        story.append(tbl)
        doc.build(story)
        pdf_bytes = buffer.getvalue()
        buffer.close()

        st.download_button(
            "PDFë¡œ ë‹¤ìš´ë¡œë“œ",
            data=pdf_bytes,
            file_name=f"{query}_results.pdf",
            mime="application/pdf",
            use_container_width=True,
        )
    # -------------------------------------------------------------------

else:
    st.info("ì¢Œì¸¡ì—ì„œ í‚¤ì›Œë“œë¥¼ ì…ë ¥ í›„ ìˆ˜ì§‘ ì‹œì‘ì„ ëˆŒëŸ¬ì£¼ì„¸ìš”.")

# -*- coding: utf-8 -*-
import io
import json
import pandas as pd
import streamlit as st
import datetime as dt

from news_aggregator import (
    collect_articles,
    enrich_with_content,
    filter_by_publishers,
    extract_domain,
    DEFAULT_RSS_FEEDS,
)

st.set_page_config(page_title="ğŸ“° ë‰´ìŠ¤ í‚¤ì›Œë“œ ìˆ˜ì§‘/ìš”ì•½ ëŒ€ì‹œë³´ë“œ", layout="wide")
st.title("ğŸ“° ë‰´ìŠ¤ í‚¤ì›Œë“œ ìˆ˜ì§‘/ìš”ì•½ ëŒ€ì‹œë³´ë“œ")
st.caption("í‚¤ì›Œë“œë¡œ ì—¬ëŸ¬ ì–¸ë¡  ê¸°ì‚¬ë¥¼ ëª¨ì•„ë³´ê³ , ë³¸ë¬¸/ìš”ì•½/í‚¤ì›Œë“œë¥¼ í•¨ê»˜ í™•ì¸í•˜ì„¸ìš”.")

# --------------------------- ì‚¬ì´ë“œë°” ---------------------------
with st.sidebar:
    st.header("ê²€ìƒ‰ ì„¤ì •")
    query = st.text_input("ê²€ìƒ‰ í‚¤ì›Œë“œ", placeholder="ì˜ˆ) ì¬ì •ì •ì±…, ë°˜ë„ì²´, í™˜ìœ¨ ê¸‰ë“±")
    max_results = st.slider("ìµœëŒ€ ê¸°ì‚¬ ìˆ˜", 10, 300, 60, step=10)
    newsapi_key = st.text_input("NewsAPI í‚¤ (ì„ íƒ)", type="password")

    st.markdown("---")
    st.subheader("ê¸°ê°„(ì„ íƒ)")
    use_date_range = st.toggle("ê¸°ê°„ í•„í„° ì‚¬ìš©", value=False)
    start_date = end_date = None
    if use_date_range:
        col1, col2 = st.columns(2)
        with col1:
            start_date = st.date_input("ì‹œì‘ì¼", value=dt.date.today())
        with col2:
            end_date = st.date_input("ì¢…ë£Œì¼", value=dt.date.today())

    st.markdown("---")
    st.subheader("RSS ì†ŒìŠ¤")
    use_default_rss = st.checkbox("ìƒ˜í”Œ ê¸°ë³¸ RSS ì‚¬ìš©", True)
    uploaded_feeds = st.file_uploader("feeds.txt ì—…ë¡œë“œ", type=["txt"])

    rss_feeds = []
    if use_default_rss:
        rss_feeds.extend(DEFAULT_RSS_FEEDS)
    if uploaded_feeds is not None:
        try:
            txt = uploaded_feeds.read().decode("utf-8", errors="ignore")
            for line in txt.splitlines():
                url = line.strip()
                if url and not url.startswith("#"):
                    rss_feeds.append(url)
        except Exception:
            st.warning("feeds.txtë¥¼ ì½ëŠ” ì¤‘ ë¬¸ì œê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤.")

    st.markdown("---")
    st.subheader("ì½˜í…ì¸  ì²˜ë¦¬")
    do_fetch_text = st.checkbox("ê¸°ì‚¬ ë³¸ë¬¸ ìˆ˜ì§‘ (í¬ë¡¤ë§)", True)
    do_summarize = st.checkbox("ìš”ì•½ ìƒì„±", True)
    summary_len = st.slider("ìš”ì•½ ë¬¸ì¥ ìˆ˜", 2, 6, 3)
    do_keywords = st.checkbox("í‚¤ì›Œë“œ(í˜•íƒœì†Œ) ì¶”ì¶œ", True)

    st.markdown("---")
    st.subheader("PDF í•œê¸€ í°íŠ¸(ì„ íƒ) ì—…ë¡œë“œ")
    pdf_font_file = st.file_uploader("NotoSansKR ë“± .ttf/.otf ì—…ë¡œë“œí•˜ë©´ PDF í•œê¸€ í‘œì‹œê°€ ì¢‹ìŠµë‹ˆë‹¤.", type=["ttf", "otf"])

    st.markdown("---")
    run = st.button("ğŸ” ìˆ˜ì§‘ ì‹œì‘", use_container_width=True)

# --------------------------- ì‹¤í–‰ ---------------------------
if run:
    if not query.strip():
        st.warning("í‚¤ì›Œë“œë¥¼ ì…ë ¥í•˜ì„¸ìš”.")
        st.stop()

    with st.spinner("ê¸°ì‚¬ë¥¼ ìˆ˜ì§‘í•˜ê³  ìˆìŠµë‹ˆë‹¤..."):
        raw = collect_articles(
            query=query,
            max_results=max_results,
            newsapi_key=newsapi_key.strip() or None,
            rss_feeds=rss_feeds if rss_feeds else None,
        )

    if not raw:
        st.info("ê´€ë ¨ ê¸°ì‚¬ë¥¼ ì°¾ì§€ ëª»í–ˆìŠµë‹ˆë‹¤.")
        st.stop()

    publishers = sorted(list({(it.get("publisher") or extract_domain(it.get("link","")) or "").strip()
                              for it in raw if it.get("link")}))
    with st.expander("ì–¸ë¡ ì‚¬/ë„ë©”ì¸ í•„í„°"):
        allow = st.multiselect("í¬í•¨í•  ì–¸ë¡ ì‚¬/ë„ë©”ì¸", options=publishers, default=[])

    filtered = filter_by_publishers(raw, allow_publishers=allow)

    # ê¸°ê°„ í•„í„°
    def parse_date_iso(s):
        try:
            return pd.to_datetime(s, utc=True).date()
        except Exception:
            return None

    if use_date_range and (start_date or end_date):
        if start_date and end_date and end_date < start_date:
            start_date, end_date = end_date, start_date
        tmp = []
        for it in filtered:
            d = parse_date_iso(it.get("published_at"))
            ok = True
            if start_date and d and d < start_date:
                ok = False
            if end_date and d and d > end_date:
                ok = False
            if ok:
                tmp.append(it)
        filtered = tmp

    if not filtered:
        st.info("í•„í„° ì¡°ê±´ì— ë§ëŠ” ê¸°ì‚¬ê°€ ì—†ìŠµë‹ˆë‹¤.")
        st.stop()

    # ë³¸ë¬¸/ìš”ì•½/í‚¤ì›Œë“œ
    if do_fetch_text or do_summarize or do_keywords:
        with st.spinner("ë³¸ë¬¸/ìš”ì•½/í‚¤ì›Œë“œë¥¼ ìƒì„± ì¤‘ì…ë‹ˆë‹¤..."):
            enriched = enrich_with_content(
                filtered,
                do_fetch_text=do_fetch_text,
                do_summarize=do_summarize,
                do_keywords=do_keywords,
                summary_sentences=summary_len,
            )
    else:
        enriched = filtered

    df = pd.DataFrame(enriched)

    # í™”ë©´ í‘œì‹œëŠ” URL ì œì™¸
    display_cols = ["title", "publisher", "published_at"]
    if do_summarize:
        display_cols.append("summary")
    if do_keywords:
        display_cols.append("keywords")

    st.success(f"ì´ {len(df)}ê±´ì˜ ê¸°ì‚¬ë¥¼ í™•ë³´í–ˆìŠµë‹ˆë‹¤.")
    st.dataframe(df[display_cols], use_container_width=True, height=520)

    # ë‚´ë¶€ ë§í¬ ì‹œë¦¬ì¦ˆ(ì—‘ì…€/PDfì—ì„œ ì œëª© í•˜ì´í¼ë§í¬ ìƒì„±ìš©)
    link_series = df["link"] if "link" in df.columns else pd.Series([""] * len(df))

    # ---------------- Excel ë‹¤ìš´ë¡œë“œ (URL ì»¬ëŸ¼ ì œì™¸, ì œëª©ë§Œ í´ë¦­ ê°€ëŠ¥) ----------------
    from io import BytesIO

    df_excel = df[display_cols].copy()  # URL ì•ˆ ë„£ìŒ

    engine = None
    try:
        import xlsxwriter
        engine = "xlsxwriter"
    except Exception:
        try:
            import openpyxl
            engine = "openpyxl"
        except Exception:
            engine = None

    if engine is None:
        st.error("xlsxwriter ë˜ëŠ” openpyxlì´ í•„ìš”í•©ë‹ˆë‹¤. requirements.txtì— ì¶”ê°€ í›„ ë°°í¬í•˜ì„¸ìš”.")
    else:
        output = BytesIO()
        with pd.ExcelWriter(output, engine=engine) as writer:
            df_excel.to_excel(writer, index=False, sheet_name="results")
            ws = writer.sheets["results"]

            cols = list(df_excel.columns)
            title_idx = cols.index("title") if "title" in cols else None

            if title_idx is not None:
                if engine == "xlsxwriter":
                    for r, (title, url) in enumerate(zip(df_excel["title"], link_series), start=1):
                        if pd.notna(url) and str(url).strip().startswith("http"):
                            ws.write_url(r, title_idx, str(url), string=str(title))
                else:
                    from openpyxl.styles import Font
                    for r, (title, url) in enumerate(zip(df_excel["title"], link_series), start=2):
                        if pd.notna(url) and str(url).strip().startswith("http"):
                            cell = ws.cell(row=r, column=title_idx + 1)
                            cell.value = str(title)
                            cell.hyperlink = str(url)
                            cell.font = Font(color="0000EE", underline="single")

        output.seek(0)
        st.download_button(
            "ì—‘ì…€(.xlsx)ë¡œ ë‹¤ìš´ë¡œë“œ",
            data=output.getvalue(),
            file_name=f"{query}_results.xlsx",
            mime="application/vnd.openxmlformats-officedocument.spreadsheetml.sheet",
            use_container_width=True,
        )
    # -------------------------------------------------------------------

    # ---------------- PDF ë‹¤ìš´ë¡œë“œ (URL ì»¬ëŸ¼ ì œì™¸, ì œëª©ë§Œ ë§í¬) ---------
    try:
        from reportlab.lib.pagesizes import A4, landscape
        from reportlab.platypus import SimpleDocTemplate, Table, TableStyle, Paragraph
        from reportlab.lib import colors
        from reportlab.lib.styles import getSampleStyleSheet, ParagraphStyle
        from reportlab.pdfbase import pdfmetrics
        from reportlab.pdfbase.ttfonts import TTFont
    except Exception:
        st.error("PDF ìƒì„±ì„ ìœ„í•´ reportlab íŒ¨í‚¤ì§€ê°€ í•„ìš”í•©ë‹ˆë‹¤. requirements.txtì— reportlabì„ ì¶”ê°€í•˜ì„¸ìš”.")
    else:
        base_font_name = "Helvetica"
        if pdf_font_file is not None:
            try:
                font_bytes = pdf_font_file.read()
                font_name = "UserKoreanFont"
                import tempfile
                with tempfile.NamedTemporaryFile(delete=False, suffix="."+pdf_font_file.name.split(".")[-1]) as tf:
                    tf.write(font_bytes)
                    tmp_font_path = tf.name
                pdfmetrics.registerFont(TTFont(font_name, tmp_font_path))
                base_font_name = font_name
            except Exception:
                st.warning("ì—…ë¡œë“œí•œ í°íŠ¸ë¥¼ ë“±ë¡í•˜ì§€ ëª»í–ˆìŠµë‹ˆë‹¤. ê¸°ë³¸ í°íŠ¸ë¡œ ì§„í–‰í•©ë‹ˆë‹¤. (í•œê¸€ í‘œì‹œê°€ ê¹¨ì§ˆ ìˆ˜ ìˆìŒ)")

        styles = getSampleStyleSheet()
        title_style = ParagraphStyle(
            name="TitleLink",
            parent=styles["Normal"],
            fontName=base_font_name,
            fontSize=10,
            textColor=colors.HexColor("#1155cc"),
            underlineProportion=0.08,
            leading=14,
        )
        cell_style = ParagraphStyle(
            name="Cell",
            parent=styles["Normal"],
            fontName=base_font_name,
            fontSize=9,
            leading=12,
        )

        rows = []
        header = ["ì œëª©", "ì–¸ë¡ ì‚¬", "ë°œí–‰ì‹œê°"]  # URL ì»¬ëŸ¼ ì œì™¸
        rows.append(header)

        for idx, row in df.iterrows():
            title = str(row.get("title", ""))
            url = str(row.get("link", "")) if pd.notna(row.get("link")) else ""
            pub = str(row.get("publisher", ""))
            when = str(row.get("published_at", ""))

            if url.startswith("http"):
                title_para = Paragraph(f'<link href="{url}">{title}</link>', title_style)
            else:
                title_para = Paragraph(title, cell_style)

            pub_para = Paragraph(pub, cell_style)
            when_para = Paragraph(when, cell_style)

            rows.append([title_para, pub_para, when_para])

        buffer = io.BytesIO()
        doc = SimpleDocTemplate(buffer, pagesize=landscape(A4), leftMargin=25, rightMargin=25, topMargin=20, bottomMargin=20)
        tbl = Table(rows, repeatRows=1, colWidths=[320, 160, 160])
        tbl.setStyle(TableStyle([
            ("BACKGROUND", (0,0), (-1,0), colors.HexColor("#f1f3f4")),
            ("TEXTCOLOR", (0,0), (-1,0), colors.HexColor("#202124")),
            ("FONTNAME", (0,0), (-1,-1), base_font_name),
            ("FONTSIZE", (0,0), (-1,0), 10),
            ("FONTSIZE", (0,1), (-1,-1), 9),
            ("ALIGN", (2,1), (2,-1), "CENTER"),
            ("VALIGN", (0,0), (-1,-1), "TOP"),
            ("ROWBACKGROUNDS", (0,1), (-1,-1), [colors.white, colors.HexColor("#fafafa")]),
            ("GRID", (0,0), (-1,-1), 0.25, colors.HexColor("#dfe1e5")),
        ]))

        story = [tbl]
        doc.build(story)
        pdf_bytes = buffer.getvalue()
        buffer.close()

        st.download_button(
            "PDFë¡œ ë‹¤ìš´ë¡œë“œ",
            data=pdf_bytes,
            file_name=f"{query}_results.pdf",
            mime="application/pdf",
            use_container_width=True,
        )
    # -------------------------------------------------------------------

else:
    st.info("ì¢Œì¸¡ì—ì„œ í‚¤ì›Œë“œë¥¼ ì…ë ¥ í›„ ìˆ˜ì§‘ ì‹œì‘ì„ ëˆŒëŸ¬ì£¼ì„¸ìš”.")

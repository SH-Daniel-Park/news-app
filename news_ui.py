# -*- coding: utf-8 -*-
import pandas as pd
import streamlit as st
import datetime as dt

from news_aggregator import (
    search_web,
    dedupe_and_sort,
    enrich_with_content,
    filter_by_domains,
)

st.set_page_config(page_title="ğŸŒ í‚¤ì›Œë“œ ì›¹ ê²€ìƒ‰/ìš”ì•½ ëŒ€ì‹œë³´ë“œ", layout="wide")

st.title("ğŸŒ í‚¤ì›Œë“œ ì›¹ ê²€ìƒ‰/ìš”ì•½ ëŒ€ì‹œë³´ë“œ")
st.caption("í‚¤ì›Œë“œì™€ ê¸°ê°„ì„ ì…ë ¥í•´ ì¼ë°˜ ì›¹ì—ì„œ ìë£Œë¥¼ ëª¨ìœ¼ê³ , ë³¸ë¬¸/ìš”ì•½/í‚¤ì›Œë“œë¥¼ í™•ì¸í•˜ì„¸ìš”.")

# --- ì…ë ¥ ì˜ì—­ (ì‚¬ì´ë“œë°”) ------------------------------------------------------
with st.sidebar:
    st.header("ê²€ìƒ‰ ì„¤ì •")
    query = st.text_input("ê²€ìƒ‰ í‚¤ì›Œë“œ", placeholder="ì˜ˆ) ìƒì„±í˜• AI ë³´ì•ˆ ê°€ì´ë“œ, ë°˜ë„ì²´ ì‹œì¥ ì „ë§")
    max_results = st.slider("ìµœëŒ€ ê²°ê³¼ ìˆ˜", 10, 300, 60, step=10)

    st.markdown("---")
    st.subheader("ê¸°ê°„")
    col1, col2 = st.columns(2)
    with col1:
        date_from = st.date_input("ì‹œì‘ì¼", value=None)
    with col2:
        date_to = st.date_input("ì¢…ë£Œì¼", value=None)

    st.markdown("---")
    st.subheader("ê²€ìƒ‰ ì—”ì§„")
    engine = st.selectbox("ì—”ì§„ ì„ íƒ", options=["DuckDuckGo(ê¸°ë³¸)"], index=0,
                          help="ê¸°ë³¸ì€ ë¬´ë£Œ/í‚¤ ë¶ˆí•„ìš”. ì›í•˜ë©´ Google/Bingë„ í™•ì¥ ê°€ëŠ¥.")

    st.markdown("---")
    st.subheader("ì½˜í…ì¸  ì²˜ë¦¬")
    do_fetch_text = st.checkbox("í˜ì´ì§€ ë³¸ë¬¸ ìˆ˜ì§‘ (í¬ë¡¤ë§)", True)
    do_summarize = st.checkbox("ìš”ì•½ ìƒì„±", True)
    summary_len = st.slider("ìš”ì•½ ë¬¸ì¥ ìˆ˜", 2, 6, 3)
    do_keywords = st.checkbox("í‚¤ì›Œë“œ(í˜•íƒœì†Œ) ì¶”ì¶œ", True)

    st.markdown("---")
    run = st.button("ğŸ” ì›¹ ê²€ìƒ‰ ì‹œì‘", use_container_width=True)

# --- ì‹¤í–‰ ----------------------------------------------------------------------
if run:
    if not query.strip():
        st.warning("í‚¤ì›Œë“œë¥¼ ì…ë ¥í•˜ì„¸ìš”.")
        st.stop()

    # ë‚ ì§œ ê²€ì¦
    if date_from and date_to and date_from > date_to:
        st.warning("ì‹œì‘ì¼ì´ ì¢…ë£Œì¼ë³´ë‹¤ ëŠ¦ìŠµë‹ˆë‹¤. í™•ì¸í•´ì£¼ì„¸ìš”.")
        st.stop()

    with st.spinner("ì›¹ì„ ê²€ìƒ‰í•˜ê³  ìˆìŠµë‹ˆë‹¤..."):
        raw = search_web(
            query=query,
            max_results=max_results,
            date_from=date_from if date_from else None,
            date_to=date_to if date_to else None,
            engine="duckduckgo",
        )

    if not raw:
        st.info("ê²€ìƒ‰ ê²°ê³¼ê°€ ì—†ìŠµë‹ˆë‹¤. í‚¤ì›Œë“œë¥¼ ë°”ê¾¸ê±°ë‚˜ ê¸°ê°„ì„ ë„“í˜€ë³´ì„¸ìš”.")
        st.stop()

    merged = dedupe_and_sort(raw)

    # ë„ë©”ì¸ í•„í„° UI
    domains = sorted(list({(it.get("domain") or "").strip() for it in merged if it.get("domain")}))
    with st.expander("ë„ë©”ì¸ í•„í„°"):
        allow = st.multiselect(
            "í¬í•¨í•  ë„ë©”ì¸ ì„ íƒ (ë¯¸ì„ íƒ ì‹œ ì „ì²´)",
            options=domains, default=[]
        )

    filtered = filter_by_domains(merged, allow_domains=allow)
    if not filtered:
        st.info("í•„í„° ì¡°ê±´ì— ë§ëŠ” ê²°ê³¼ê°€ ì—†ìŠµë‹ˆë‹¤. í•„í„°ë¥¼ ë¹„ìš°ê±°ë‚˜ ë³€ê²½í•´ ë³´ì„¸ìš”.")
        st.stop()

    # ë³¸ë¬¸/ìš”ì•½/í‚¤ì›Œë“œ ì¶”ê°€
    if do_fetch_text or do_summarize or do_keywords:
        with st.spinner("ë³¸ë¬¸/ìš”ì•½/í‚¤ì›Œë“œë¥¼ ìƒì„± ì¤‘ì…ë‹ˆë‹¤..."):
            enriched = enrich_with_content(
                filtered,
                do_fetch_text=do_fetch_text,
                do_summarize=do_summarize,
                do_keywords=do_keywords,
                summary_sentences=summary_len,
            )
    else:
        enriched = filtered

    # í‘œ í‘œì‹œ ---------------------------------------------------------------
    df = pd.DataFrame(enriched)
    display_cols = ["title", "domain", "published_at", "link"]
    if do_summarize:
        display_cols.append("summary")
    if do_keywords:
        display_cols.append("keywords")

    st.success(f"ì´ {len(df)}ê±´ì˜ ê²°ê³¼ë¥¼ í™•ë³´í–ˆìŠµë‹ˆë‹¤.")

    # ë§í¬ í´ë¦­ ê°€ëŠ¥ (í•œ ë²ˆ í´ë¦­ìœ¼ë¡œ ìƒˆ íƒ­ ì´ë™)
    # ë§í¬ë¥¼ 'https://...' ë¬¸ìì—´ë¡œ í‘œì— ì§ì ‘ í¬í•¨
    df_display = df[display_cols].copy()
    # ë§í¬ ì—´ì´ ì¡´ì¬í•˜ë©´, ë¬¸ìì—´ ê·¸ëŒ€ë¡œ (https://...) í˜•ì‹
    if "link" in df_display.columns:
        df_display["link"] = df_display["link"].astype(str)
    st.dataframe(
        df_display,
        use_container_width=True,
        height=520
    )

    # ìƒì„¸ ë³´ê¸° --------------------------------------------------------------
    st.markdown("### ì„¸ë¶€ ë³´ê¸°")
    titles = ["(ì„ íƒ)"] + df["title"].tolist()
    sel = st.selectbox("ë³¸ë¬¸/ìš”ì•½/í‚¤ì›Œë“œë¥¼ í™•ì¸í•  í•­ëª©", options=titles, index=0)
    if sel != "(ì„ íƒ)":
        row = df[df["title"] == sel].iloc[0]
        st.markdown(f"**ë„ë©”ì¸**: {row.get('domain','')}  |  **ì‹œê°**: {row.get('published_at','')}")
        st.markdown(f"**ì›ë¬¸ ë§í¬**: {row.get('link','')}")
        if do_fetch_text:
            st.markdown("#### ë³¸ë¬¸")
            st.write(row.get("content", "") or "ë³¸ë¬¸ì„ ìˆ˜ì§‘í•˜ì§€ ëª»í–ˆìŠµë‹ˆë‹¤.")
        if do_summarize:
            st.markdown("#### ìš”ì•½")
            st.write(row.get("summary", "") or "-")
        if do_keywords:
            st.markdown("#### í‚¤ì›Œë“œ")
            kw = row.get("keywords", []) or []
            st.write(", ".join(kw) if kw else "-")

    # CSV ë‹¤ìš´ë¡œë“œ -----------------------------------------------------------
    st.markdown("---")
    st.subheader("ê²°ê³¼ ë‹¤ìš´ë¡œë“œ")
    csv_bytes = df.to_csv(index=False, encoding="utf-8-sig").encode("utf-8-sig")
    st.download_button(
        "CSVë¡œ ë‹¤ìš´ë¡œë“œ",
        data=csv_bytes,
        file_name=f"{query}_web.csv",
        mime="text/csv",
        use_container_width=True,
    )

else:
    st.info("ì¢Œì¸¡ ì‚¬ì´ë“œë°”ì—ì„œ í‚¤ì›Œë“œì™€ ê¸°ê°„ì„ ì„¤ì •í•˜ê³  **ì›¹ ê²€ìƒ‰ ì‹œì‘**ì„ ëˆŒëŸ¬ì£¼ì„¸ìš”.")

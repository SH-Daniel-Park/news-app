# -*- coding: utf-8 -*-
import pandas as pd
import streamlit as st
import datetime as dt

from news_aggregator import (
    search_web,
    dedupe_and_sort,
    filter_by_domains,
)

st.set_page_config(page_title="ğŸŒ í‚¤ì›Œë“œ ì›¹ ê²€ìƒ‰/ìš”ì•½ ëŒ€ì‹œë³´ë“œ (RL Fix)", layout="wide")

st.title("ğŸŒ í‚¤ì›Œë“œ ì›¹ ê²€ìƒ‰/ìš”ì•½ ëŒ€ì‹œë³´ë“œ (RL Fix)")
st.caption("DuckDuckGo ë ˆì´íŠ¸ë¦¬ë°‹ì„ ì§€ìˆ˜ ë°±ì˜¤í”„ë¡œ ì™„í™”í•˜ê³ , Bing API(ì„ íƒ)ë¥¼ í´ë°±ìœ¼ë¡œ ì§€ì›í•©ë‹ˆë‹¤.")

with st.sidebar:
    st.header("ê²€ìƒ‰ ì„¤ì •")
    query = st.text_input("ê²€ìƒ‰ í‚¤ì›Œë“œ", placeholder="ì˜ˆ) ìƒì„±í˜• AI ë³´ì•ˆ ê°€ì´ë“œ, ë°˜ë„ì²´ ì‹œì¥ ì „ë§")
    max_results = st.slider("ìµœëŒ€ ê²°ê³¼ ìˆ˜", 10, 100, 40, step=10,
                            help="ë ˆì´íŠ¸ë¦¬ë°‹ ì™„í™”ë¥¼ ìœ„í•´ 40~50 ì´í•˜ë¥¼ ê¶Œì¥")

    st.markdown("---")
    st.subheader("ê¸°ê°„")
    col1, col2 = st.columns(2)
    with col1:
        date_from = st.date_input("ì‹œì‘ì¼", value=None)
    with col2:
        date_to = st.date_input("ì¢…ë£Œì¼", value=None)

    st.markdown("---")
    st.subheader("ì—”ì§„")
    engine = st.selectbox("ê²€ìƒ‰ ì—”ì§„", options=["DuckDuckGo(ë¬´ë£Œ)", "Bing(ì•ˆì •/í‚¤í•„ìš”)"], index=0)
    bing_api_key = ""
    if engine.startswith("Bing"):
        bing_api_key = st.text_input("Bing API í‚¤", type="password",
                                     help="Azure Bing Web Search API í‚¤ ì…ë ¥ ì‹œ ë³´ë‹¤ ì•ˆì •ì ìœ¼ë¡œ ì‘ë™í•©ë‹ˆë‹¤.")

    st.markdown("---")
    run = st.button("ğŸ” ì›¹ ê²€ìƒ‰ ì‹œì‘", use_container_width=True)

# ê²°ê³¼ ìºì‹œ (ì¿¼ë¦¬/ê¸°ê°„/ì—”ì§„ë³„ 15ë¶„ ìºì‹œ)
@st.cache_data(show_spinner=False, ttl=900)
def _cached_search(query, max_results, date_from, date_to, engine, bing_api_key):
    eng = "bing" if engine.startswith("Bing") else "duckduckgo"
    return search_web(
        query=query,
        max_results=max_results,
        date_from=date_from or None,
        date_to=date_to or None,
        engine=eng,
        bing_api_key=bing_api_key or None,
    )

if run:
    if not query.strip():
        st.warning("í‚¤ì›Œë“œë¥¼ ì…ë ¥í•˜ì„¸ìš”.")
        st.stop()

    if date_from and date_to and date_from > date_to:
        st.warning("ì‹œì‘ì¼ì´ ì¢…ë£Œì¼ë³´ë‹¤ ëŠ¦ìŠµë‹ˆë‹¤.")
        st.stop()

    with st.spinner("ì›¹ì„ ê²€ìƒ‰í•˜ê³  ìˆìŠµë‹ˆë‹¤..."):
        raw = _cached_search(query, max_results, date_from, date_to, engine, bing_api_key)

    if not raw:
        st.info("ê²€ìƒ‰ ê²°ê³¼ê°€ ì—†ìŠµë‹ˆë‹¤. í‚¤ì›Œë“œë¥¼ ë°”ê¾¸ê±°ë‚˜ ê¸°ê°„ì„ ë„“í˜€ë³´ì„¸ìš”.\n\nBing API í‚¤ ì‚¬ìš©ë„ ê³ ë ¤í•´ ë³´ì„¸ìš”.")
        st.stop()

    merged = dedupe_and_sort(raw)

    domains = sorted(list({(it.get("domain") or "").strip() for it in merged if it.get("domain")}))
    with st.expander("ë„ë©”ì¸ í•„í„°"):
        allow = st.multiselect("í¬í•¨í•  ë„ë©”ì¸ ì„ íƒ (ë¯¸ì„ íƒ ì‹œ ì „ì²´)", options=domains, default=[])

    filtered = filter_by_domains(merged, allow_domains=allow)
    if not filtered:
        st.info("í•„í„° ì¡°ê±´ì— ë§ëŠ” ê²°ê³¼ê°€ ì—†ìŠµë‹ˆë‹¤.")
        st.stop()

    df = pd.DataFrame(filtered)
    display_cols = ["title", "domain", "published_at", "link"]

    st.success(f"ì´ {len(df)}ê±´ì˜ ê²°ê³¼ë¥¼ í™•ë³´í–ˆìŠµë‹ˆë‹¤.")

    # URLì€ 'https://...' ë¬¸ìì—´ë¡œ ê·¸ëŒ€ë¡œ í‘œì‹œ
    df_display = df[display_cols].copy()
    if "link" in df_display.columns:
        df_display["link"] = df_display["link"].astype(str)
    st.dataframe(df_display, use_container_width=True, height=520)

    st.markdown("---")
    st.subheader("ê²°ê³¼ ë‹¤ìš´ë¡œë“œ")
    csv_bytes = df_display.to_csv(index=False, encoding="utf-8-sig").encode("utf-8-sig")
    st.download_button("CSVë¡œ ë‹¤ìš´ë¡œë“œ", data=csv_bytes, file_name=f"{query}_web.csv",
                       mime="text/csv", use_container_width=True)

else:
    st.info("ì¢Œì¸¡ ì‚¬ì´ë“œë°”ì—ì„œ í‚¤ì›Œë“œ/ê¸°ê°„ì„ ì„¤ì •í•˜ê³  **ì›¹ ê²€ìƒ‰ ì‹œì‘**ì„ ëˆŒëŸ¬ì£¼ì„¸ìš”.\n\n"
            "ë ˆì´íŠ¸ë¦¬ë°‹ì´ ì¦ë‹¤ë©´ ê²°ê³¼ ìˆ˜ë¥¼ ì¤„ì´ê±°ë‚˜, Bing API í‚¤ë¥¼ ì‚¬ìš©í•´ ë³´ì„¸ìš”.")

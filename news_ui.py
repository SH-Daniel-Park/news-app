# -*- coding: utf-8 -*-
import pandas as pd
import streamlit as st
import datetime as dt

from news_aggregator import (
    search_web,
    dedupe_and_sort,
    enrich_with_content,
    filter_by_domains,
)

st.set_page_config(page_title="ğŸŒ í‚¤ì›Œë“œ ì›¹ ê²€ìƒ‰/ìš”ì•½ ëŒ€ì‹œë³´ë“œ (ìš¸íŠ¸ë¼ë¼ì´íŠ¸)", layout="wide")

st.title("ğŸŒ í‚¤ì›Œë“œ ì›¹ ê²€ìƒ‰/ìš”ì•½ ëŒ€ì‹œë³´ë“œ (ìš¸íŠ¸ë¼ë¼ì´íŠ¸)")
st.caption("ì˜ì¡´ì„±ì„ ìµœì†Œí™”í•œ ë°°í¬ìš© ë²„ì „ì…ë‹ˆë‹¤. ë³¸ë¬¸ ìˆ˜ì§‘ì€ ë¹„í™œì„±í™”ë˜ì–´ ìˆìŠµë‹ˆë‹¤.")

with st.sidebar:
    st.header("ê²€ìƒ‰ ì„¤ì •")
    query = st.text_input("ê²€ìƒ‰ í‚¤ì›Œë“œ", placeholder="ì˜ˆ) ìƒì„±í˜• AI ë³´ì•ˆ ê°€ì´ë“œ, ë°˜ë„ì²´ ì‹œì¥ ì „ë§")
    max_results = st.slider("ìµœëŒ€ ê²°ê³¼ ìˆ˜", 10, 300, 60, step=10)

    st.markdown("---")
    st.subheader("ê¸°ê°„")
    col1, col2 = st.columns(2)
    with col1:
        date_from = st.date_input("ì‹œì‘ì¼", value=None)
    with col2:
        date_to = st.date_input("ì¢…ë£Œì¼", value=None)

    st.markdown("---")
    st.subheader("ì½˜í…ì¸  ì²˜ë¦¬")
    do_fetch_text = st.checkbox("í˜ì´ì§€ ë³¸ë¬¸ ìˆ˜ì§‘", False, help="ìš¸íŠ¸ë¼ë¼ì´íŠ¸ ë²„ì „ì—ì„œëŠ” ë¹„í™œì„±í™”ë¨")
    do_summarize = st.checkbox("ìš”ì•½ ìƒì„±", False, help="ë³¸ë¬¸ ìˆ˜ì§‘ì´ êº¼ì ¸ ìˆìœ¼ë©´ ìš”ì•½ë„ ìƒì„±ë˜ì§€ ì•ŠìŠµë‹ˆë‹¤")
    summary_len = st.slider("ìš”ì•½ ë¬¸ì¥ ìˆ˜", 2, 6, 3, disabled=not do_summarize)
    do_keywords = st.checkbox("í‚¤ì›Œë“œ(í˜•íƒœì†Œ) ì¶”ì¶œ", False, help="ë³¸ë¬¸ ìˆ˜ì§‘ì´ êº¼ì ¸ ìˆìœ¼ë©´ í‚¤ì›Œë“œë„ ìƒì„±ë˜ì§€ ì•ŠìŠµë‹ˆë‹¤")

    st.markdown("---")
    run = st.button("ğŸ” ì›¹ ê²€ìƒ‰ ì‹œì‘", use_container_width=True)

if run:
    if not query.strip():
        st.warning("í‚¤ì›Œë“œë¥¼ ì…ë ¥í•˜ì„¸ìš”.")
        st.stop()

    if date_from and date_to and date_from > date_to:
        st.warning("ì‹œì‘ì¼ì´ ì¢…ë£Œì¼ë³´ë‹¤ ëŠ¦ìŠµë‹ˆë‹¤.")
        st.stop()

    with st.spinner("ì›¹ì„ ê²€ìƒ‰í•˜ê³  ìˆìŠµë‹ˆë‹¤..."):
        raw = search_web(
            query=query,
            max_results=max_results,
            date_from=date_from or None,
            date_to=date_to or None,
        )

    if not raw:
        st.info("ê²€ìƒ‰ ê²°ê³¼ê°€ ì—†ìŠµë‹ˆë‹¤. í‚¤ì›Œë“œë¥¼ ë°”ê¾¸ê±°ë‚˜ ê¸°ê°„ì„ ë„“í˜€ë³´ì„¸ìš”.")
        st.stop()

    merged = dedupe_and_sort(raw)

    domains = sorted(list({(it.get("domain") or "").strip() for it in merged if it.get("domain")}))
    with st.expander("ë„ë©”ì¸ í•„í„°"):
        allow = st.multiselect("í¬í•¨í•  ë„ë©”ì¸ ì„ íƒ (ë¯¸ì„ íƒ ì‹œ ì „ì²´)", options=domains, default=[])

    filtered = filter_by_domains(merged, allow_domains=allow)
    if not filtered:
        st.info("í•„í„° ì¡°ê±´ì— ë§ëŠ” ê²°ê³¼ê°€ ì—†ìŠµë‹ˆë‹¤.")
        st.stop()

    df = pd.DataFrame(filtered)
    display_cols = ["title", "domain", "published_at", "link"]
    st.success(f"ì´ {len(df)}ê±´ì˜ ê²°ê³¼ë¥¼ í™•ë³´í–ˆìŠµë‹ˆë‹¤.")

    # ë§í¬ëŠ” 'https://...' í˜•ì‹ì˜ ë¬¸ìì—´ë¡œ ê·¸ëŒ€ë¡œ í‘œì‹œ
    df_display = df[display_cols].copy()
    if "link" in df_display.columns:
        df_display["link"] = df_display["link"].astype(str)

    st.dataframe(df_display, use_container_width=True, height=520)

    st.markdown("### ì„¸ë¶€ ë³´ê¸° (ìš¸íŠ¸ë¼ë¼ì´íŠ¸: ë³¸ë¬¸/ìš”ì•½/í‚¤ì›Œë“œ ë¹„í™œì„±)")
    titles = ["(ì„ íƒ)"] + df["title"].tolist()
    sel = st.selectbox("í•­ëª© ì„ íƒ", options=titles, index=0)
    if sel != "(ì„ íƒ)":
        row = df[df["title"] == sel].iloc[0]
        st.markdown(f"**ë„ë©”ì¸**: {row.get('domain','')}  |  **ì‹œê°**: {row.get('published_at','')}")
        st.markdown(f"**ì›ë¬¸ ë§í¬**: {row.get('link','')}")

    st.markdown("---")
    st.subheader("ê²°ê³¼ ë‹¤ìš´ë¡œë“œ")
    csv_bytes = df_display.to_csv(index=False, encoding="utf-8-sig").encode("utf-8-sig")
    st.download_button("CSVë¡œ ë‹¤ìš´ë¡œë“œ", data=csv_bytes, file_name=f"{query}_web.csv", mime="text/csv", use_container_width=True)

else:
    st.info("ì¢Œì¸¡ ì‚¬ì´ë“œë°”ì—ì„œ í‚¤ì›Œë“œì™€ ê¸°ê°„ì„ ì„¤ì •í•˜ê³  **ì›¹ ê²€ìƒ‰ ì‹œì‘**ì„ ëˆŒëŸ¬ì£¼ì„¸ìš”.")
